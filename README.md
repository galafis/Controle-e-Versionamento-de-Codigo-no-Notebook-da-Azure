# Controle e Versionamento de C√≥digo no Notebook da Azure

Este projeto, proposto pela **DIO** e feito por mim, demonstra como utilizar o Azure Databricks para versionamento e organiza√ß√£o de notebooks em ambientes de dados. A proposta inclui a cria√ß√£o de clusters, importa√ß√£o de arquivos, execu√ß√£o de notebooks com aux√≠lio de intelig√™ncia artificial, al√©m da integra√ß√£o com Azure DevOps para controle de c√≥digo e automa√ß√£o de esteiras de CI/CD. √â apresentado o uso pr√°tico da IA integrada ao Databricks para gera√ß√£o de c√≥digo em Python e Spark, facilitando a cria√ß√£o de notebooks interativos com filtros, sumariza√ß√µes, visualiza√ß√µes e coment√°rios explicativos. Tamb√©m s√£o exploradas boas pr√°ticas de organiza√ß√£o, exporta√ß√£o e reaproveitamento de notebooks, bem como o uso de recursos do Microsoft Learn, que oferecem exerc√≠cios guiados e roteiros de aprendizado. A abordagem permite trabalhar de forma colaborativa, segura e com versionamento estruturado em ambientes de an√°lise, engenharia de dados e machine learning dentro da plataforma Azure.

## Insights e Possibilidades

### Integra√ß√£o com DevOps
A integra√ß√£o do Azure Databricks com o Azure DevOps proporciona um fluxo de trabalho completo para desenvolvimento, teste e implanta√ß√£o de notebooks. Isso permite:

- Controle de vers√£o eficiente dos notebooks
- Automa√ß√£o de testes e valida√ß√£o de c√≥digo
- Implanta√ß√£o cont√≠nua em ambientes de produ√ß√£o
- Rastreabilidade de altera√ß√µes e colabora√ß√£o entre equipes

### Uso de IA para Desenvolvimento
A integra√ß√£o de IA no Databricks revoluciona o desenvolvimento de notebooks:

```python
# Exemplo de uso de IA para gerar c√≥digo Spark
# Basta descrever o que voc√™ precisa em um coment√°rio:

# Gerar c√≥digo para ler dados CSV, filtrar registros com valores nulos e calcular estat√≠sticas b√°sicas
from pyspark.sql.functions import col, count, when, isnan, avg, min, max

# O c√≥digo abaixo seria sugerido pela IA
df = spark.read.format("csv").option("header", "true").load("/path/to/data.csv")
df_filtered = df.filter(~col("column_name").isNull())
df_stats = df_filtered.select(
    count("*").alias("total_records"),
    avg("numeric_column").alias("average_value"),
    min("numeric_column").alias("min_value"),
    max("numeric_column").alias("max_value")
)
display(df_stats)
```

### Organiza√ß√£o e Reaproveitamento
A estrutura√ß√£o adequada de notebooks permite:

- Cria√ß√£o de bibliotecas de fun√ß√µes reutiliz√°veis
- Modulariza√ß√£o de c√≥digo para manuten√ß√£o simplificada
- Compartilhamento de componentes entre projetos
- Documenta√ß√£o integrada e autoexplicativa

## Processo de Trabalho com Azure Databricks

### 1. Cria√ß√£o e Configura√ß√£o de Clusters

O processo come√ßa com a cria√ß√£o de clusters otimizados para as necessidades espec√≠ficas do projeto:

```python
# Configura√ß√£o program√°tica de cluster via API
from databricks.sdk import WorkspaceClient
from databricks.sdk.service import compute

w = WorkspaceClient()

cluster_config = compute.ClusterSpec(
    cluster_name="MeuClusterDatabricks",
    spark_version="11.3.x-scala2.12",
    node_type_id="Standard_DS3_v2",
    autoscale=compute.AutoScale(min_workers=1, max_workers=5),
    spark_conf={
        "spark.databricks.delta.preview.enabled": "true",
        "spark.databricks.io.cache.enabled": "true"
    },
    autotermination_minutes=60
)

cluster = w.clusters.create(cluster_config)
print(f"Cluster criado com ID: {cluster.cluster_id}")
```

### 2. Importa√ß√£o e Organiza√ß√£o de Arquivos

A importa√ß√£o de arquivos pode ser feita via interface ou programaticamente:

```python
# Importa√ß√£o de arquivos para o DBFS
dbutils.fs.cp("file:/local/path/arquivo.csv", "dbfs:/FileStore/datasets/arquivo.csv")

# Listagem de arquivos importados
files = dbutils.fs.ls("dbfs:/FileStore/datasets/")
display(files)
```

### 3. Versionamento com Git e Azure DevOps

Integra√ß√£o com reposit√≥rios Git para controle de vers√£o:

```bash
# Comandos executados no terminal do Databricks
git init
git remote add origin https://dev.azure.com/minha-org/meu-projeto/_git/databricks-notebooks
git add .
git commit -m "Vers√£o inicial dos notebooks"
git push -u origin master
```

No notebook Databricks:
```python
# Verifica√ß√£o do status do reposit√≥rio
%sh
git status
git log --oneline -5
```

### 4. Automa√ß√£o de CI/CD com Azure DevOps

Exemplo de pipeline YAML para automa√ß√£o:

```yaml
# azure-pipelines.yml
trigger:
- main

pool:
  vmImage: 'ubuntu-latest'

steps:
- task: UsePythonVersion@0
  inputs:
    versionSpec: '3.8'
    addToPath: true

- script: |
    pip install databricks-cli pytest
    databricks configure --token
    echo $(DATABRICKS_TOKEN) | databricks configure --token
  displayName: 'Configurar CLI do Databricks'

- script: |
    databricks workspace import_dir ./notebooks /Shared/Projeto
  displayName: 'Implantar notebooks'

- script: |
    pytest ./tests
  displayName: 'Executar testes'
```

### 5. Execu√ß√£o de Notebooks com Par√¢metros

Execu√ß√£o parametrizada para flexibilidade:

```python
# Defini√ß√£o de widgets para par√¢metros
dbutils.widgets.text("data_inicio", "2023-01-01", "Data In√≠cio")
dbutils.widgets.text("data_fim", "2023-12-31", "Data Fim")

# Recupera√ß√£o dos par√¢metros
data_inicio = dbutils.widgets.get("data_inicio")
data_fim = dbutils.widgets.get("data_fim")

# Uso dos par√¢metros na consulta
df = spark.sql(f"""
SELECT *
FROM tabela_dados
WHERE data BETWEEN '{data_inicio}' AND '{data_fim}'
""")

display(df)
```

### 6. Visualiza√ß√µes e An√°lises Interativas

Cria√ß√£o de visualiza√ß√µes avan√ßadas:

```python
# Importa√ß√£o de bibliotecas para visualiza√ß√£o
import matplotlib.pyplot as plt
import seaborn as sns

# Prepara√ß√£o dos dados
resultados = spark.sql("""
SELECT categoria, SUM(valor) as total
FROM vendas
GROUP BY categoria
ORDER BY total DESC
LIMIT 10
""").toPandas()

# Cria√ß√£o de visualiza√ß√£o
plt.figure(figsize=(10, 6))
sns.barplot(x='categoria', y='total', data=resultados)
plt.title('Total de Vendas por Categoria')
plt.xticks(rotation=45)
plt.tight_layout()
display()
```

## Recursos de Aprendizado

O Microsoft Learn oferece recursos valiosos para aprofundar o conhecimento em Azure Databricks:

- [Introdu√ß√£o ao Azure Databricks](https://learn.microsoft.com/pt-br/training/modules/intro-to-azure-databricks/)
- [Engenharia de Dados com Azure Databricks](https://learn.microsoft.com/pt-br/training/paths/data-engineer-azure-databricks/)
- [Ci√™ncia de Dados com Azure Databricks](https://learn.microsoft.com/pt-br/training/paths/perform-data-science-azure-databricks/)

## Conclus√£o

O Azure Databricks, quando combinado com pr√°ticas adequadas de versionamento e organiza√ß√£o de c√≥digo, proporciona um ambiente robusto e colaborativo para projetos de dados. A integra√ß√£o com Azure DevOps e o uso de IA para assist√™ncia no desenvolvimento elevam a produtividade e a qualidade dos notebooks, permitindo que equipes trabalhem de forma mais eficiente em projetos complexos de an√°lise, engenharia de dados e machine learning.


## üìã Descri√ß√£o

Descreva aqui o conte√∫do desta se√ß√£o.


## üì¶ Instala√ß√£o

Descreva aqui o conte√∫do desta se√ß√£o.


## üíª Uso

Descreva aqui o conte√∫do desta se√ß√£o.


## üìÑ Licen√ßa

Descreva aqui o conte√∫do desta se√ß√£o.
