<div align="center">

# Controle e Versionamento de C√≥digo no Notebook da Azure

[English Version](./README.md#english-version)

</div>

## üñºÔ∏è Imagem Hero

![Imagem Hero do Projeto](./hero_image_no_text.png)

Este projeto, proposto pela **DIO** e feito por mim, demonstra como utilizar o Azure Databricks para versionamento e organiza√ß√£o de notebooks em ambientes de dados. A proposta inclui a cria√ß√£o de clusters, importa√ß√£o de arquivos, execu√ß√£o de notebooks com aux√≠lio de intelig√™ncia artificial, al√©m da integra√ß√£o com Azure DevOps para controle de c√≥digo e automa√ß√£o de esteiras de CI/CD. √â apresentado o uso pr√°tico da IA integrada ao Databricks para gera√ß√£o de c√≥digo em Python e Spark, facilitando a cria√ß√£o de notebooks interativos com filtros, sumariza√ß√µes, visualiza√ß√µes e coment√°rios explicativos. Tamb√©m s√£o exploradas boas pr√°ticas de organiza√ß√£o, exporta√ß√£o e reaproveitamento de notebooks, bem como o uso de recursos do Microsoft Learn, que oferecem exerc√≠cios guiados e roteiros de aprendizado. A abordagem permite trabalhar de forma colaborativa, segura e com versionamento estruturado em ambientes de an√°lise, engenharia de dados e machine learning dentro da plataforma Azure.

## Insights e Possibilidades

### Integra√ß√£o com DevOps
A integra√ß√£o do Azure Databricks com o Azure DevOps proporciona um fluxo de trabalho completo para desenvolvimento, teste e implanta√ß√£o de notebooks. Isso permite:

- Controle de vers√£o eficiente dos notebooks
- Automa√ß√£o de testes e valida√ß√£o de c√≥digo
- Implanta√ß√£o cont√≠nua em ambientes de produ√ß√£o
- Rastreabilidade de altera√ß√µes e colabora√ß√£o entre equipes

### Uso de IA para Desenvolvimento
A integra√ß√£o de IA no Databricks revoluciona o desenvolvimento de notebooks:

```python
# Exemplo de uso de IA para gerar c√≥digo Spark
# Basta descrever o que voc√™ precisa em um coment√°rio:

# Gerar c√≥digo para ler dados CSV, filtrar registros com valores nulos e calcular estat√≠sticas b√°sicas
from pyspark.sql.functions import col, count, when, isnan, avg, min, max

# O c√≥digo abaixo seria sugerido pela IA
df = spark.read.format("csv").option("header", "true").load("/path/to/data.csv")
df_filtered = df.filter(~col("column_name").isNull())
df_stats = df_filtered.select(
    count("*").alias("total_records"),
    avg("numeric_column").alias("average_value"),
    min("numeric_column").alias("min_value"),
    max("numeric_column").alias("max_value")
)
display(df_stats)
```

### Organiza√ß√£o e Reaproveitamento
A estrutura√ß√£o adequada de notebooks permite:

- Cria√ß√£o de bibliotecas de fun√ß√µes reutiliz√°veis
- Modulariza√ß√£o de c√≥digo para manuten√ß√£o simplificada
- Compartilhamento de componentes entre projetos
- Documenta√ß√£o integrada e autoexplicativa

## Processo de Trabalho com Azure Databricks

### 1. Cria√ß√£o e Configura√ß√£o de Clusters

O processo come√ßa com a cria√ß√£o de clusters otimizados para as necessidades espec√≠ficas do projeto:

```python
# Configura√ß√£o program√°tica de cluster via API
from databricks.sdk import WorkspaceClient
from databricks.sdk.service import compute

w = WorkspaceClient()

cluster_config = compute.ClusterSpec(
    cluster_name="MeuClusterDatabricks",
    spark_version="11.3.x-scala2.12",
    node_type_id="Standard_DS3_v2",
    autoscale=compute.AutoScale(min_workers=1, max_workers=5),
    spark_conf={
        "spark.databricks.delta.preview.enabled": "true",
        "spark.databricks.io.cache.enabled": "true"
    },
    autotermination_minutes=60
)

cluster = w.clusters.create(cluster_config)
print(f"Cluster criado com ID: {cluster.cluster_id}")
```

### 2. Importa√ß√£o e Organiza√ß√£o de Arquivos

A importa√ß√£o de arquivos pode ser feita via interface ou programaticamente:

```python
# Importa√ß√£o de arquivos para o DBFS
dbutils.fs.cp("file:/local/path/arquivo.csv", "dbfs:/FileStore/datasets/arquivo.csv")

# Listagem de arquivos importados
files = dbutils.fs.ls("dbfs:/FileStore/datasets/")
display(files)
```

### 3. Versionamento com Git e Azure DevOps

Integra√ß√£o com reposit√≥rios Git para controle de vers√£o:

```bash
# Comandos executados no terminal do Databricks
git init
git remote add origin https://dev.azure.com/minha-org/meu-projeto/_git/databricks-notebooks
git add .
git commit -m "Vers√£o inicial dos notebooks"
git push -u origin master
```

No notebook Databricks:
```python
# Verifica√ß√£o do status do reposit√≥rio
%sh
git status
git log --oneline -5
```

### 4. Automa√ß√£o de CI/CD com Azure DevOps

Exemplo de pipeline YAML para automa√ß√£o:

```yaml
# azure-pipelines.yml
trigger:
- main

pool:
  vmImage: "ubuntu-latest"

steps:
- task: UsePythonVersion@0
  inputs:
    versionSpec: "3.8"
    addToPath: true

- script: |
    pip install databricks-cli pytest
    databricks configure --token
    echo $(DATABRICKS_TOKEN) | databricks configure --token
  displayName: "Configurar CLI do Databricks"

- script: |
    databricks workspace import_dir ./notebooks /Shared/Projeto
  displayName: "Implantar notebooks"

- script: |
    pytest ./tests
  displayName: "Executar testes"
```

### 5. Execu√ß√£o de Notebooks com Par√¢metros

Execu√ß√£o parametrizada para flexibilidade:

```python
# Defini√ß√£o de widgets para par√¢metros
dbutils.widgets.text("data_inicio", "2023-01-01", "Data In√≠cio")
dbutils.widgets.text("data_fim", "2023-12-31", "Data Fim")

# Recupera√ß√£o dos par√¢metros
data_inicio = dbutils.widgets.get("data_inicio")
data_fim = dbutils.widgets.get("data_fim")

# Uso dos par√¢metros na consulta
df = spark.sql(f"""
SELECT *
FROM tabela_dados
WHERE data BETWEEN \'{data_inicio}\' AND \'{data_fim}\'
""")

display(df)
```

### 6. Visualiza√ß√µes e An√°lises Interativas

Cria√ß√£o de visualiza√ß√µes avan√ßadas:

```python
# Importa√ß√£o de bibliotecas para visualiza√ß√£o
import matplotlib.pyplot as plt
import seaborn as sns

# Prepara√ß√£o dos dados
resultados = spark.sql("""
SELECT categoria, SUM(valor) as total
FROM vendas
GROUP BY categoria
ORDER BY total DESC
LIMIT 10
""").toPandas()

# Cria√ß√£o de visualiza√ß√£o
plt.figure(figsize=(10, 6))
sns.barplot(x=\'categoria\', y=\'total\', data=resultados)
plt.title(\'Total de Vendas por Categoria\')
plt.xticks(rotation=45)
plt.tight_layout()
display()
```

## Recursos de Aprendizado

O Microsoft Learn oferece recursos valiosos para aprofundar o conhecimento em Azure Databricks:

- [Introdu√ß√£o ao Azure Databricks](https://learn.microsoft.com/pt-br/training/modules/intro-to-azure-databricks/)
- [Engenharia de Dados com Azure Databricks](https://learn.microsoft.com/pt-br/training/paths/data-engineer-azure-databricks/)
- [Ci√™ncia de Dados com Azure Databricks](https://learn.microsoft.com/pt-br/training/paths/perform-data-science-azure-databricks/)

## Conclus√£o

O Azure Databricks, quando combinado com pr√°ticas adequadas de versionamento e organiza√ß√£o de c√≥digo, proporciona um ambiente robusto e colaborativo para projetos de dados. A integra√ß√£o com Azure DevOps e o uso de IA para assist√™ncia no desenvolvimento elevam a produtividade e a qualidade dos notebooks, permitindo que equipes trabalhem de forma mais eficiente em projetos complexos de an√°lise, engenharia de dados e machine learning.


## üìã Descri√ß√£o

Descreva aqui o conte√∫do desta se√ß√£o.


## üì¶ Instala√ß√£o

Descreva aqui o conte√∫do desta se√ß√£o.


## üíª Uso

Descreva aqui o conte√∫do desta se√ß√£o.


## üìÑ Licen√ßa

Descreva aqui o conte√∫do desta se√ß√£o.

---

<div align="center">

# English Version

[Vers√£o em Portugu√™s](./README.md#controle-e-versionamento-de-c√≥digo-no-notebook-da-azure)

</div>

## üñºÔ∏è Hero Image

![Project Hero Image](./hero_image_no_text.png)

This project, proposed by **DIO** and developed by me, demonstrates how to use Azure Databricks for versioning and organizing notebooks in data environments. The proposal includes creating clusters, importing files, executing notebooks with the help of artificial intelligence, and integrating with Azure DevOps for code control and CI/CD pipeline automation. It presents the practical use of AI integrated into Databricks for generating Python and Spark code, facilitating the creation of interactive notebooks with filters, summaries, visualizations, and explanatory comments. Best practices for organizing, exporting, and reusing notebooks are also explored, as well as the use of Microsoft Learn resources, which offer guided exercises and learning paths. The approach allows for collaborative, secure, and structured versioning in data analysis, data engineering, and machine learning environments within the Azure platform.

## Insights and Possibilities

### DevOps Integration
Azure Databricks integration with Azure DevOps provides a complete workflow for developing, testing, and deploying notebooks. This allows for:

- Efficient version control of notebooks
- Automation of code testing and validation
- Continuous deployment in production environments
- Traceability of changes and collaboration between teams

### Using AI for Development
AI integration in Databricks revolutionizes notebook development:

```python
# Example of using AI to generate Spark code
# Just describe what you need in a comment:

# Generate code to read CSV data, filter records with null values, and calculate basic statistics
from pyspark.sql.functions import col, count, when, isnan, avg, min, max

# The code below would be suggested by AI
df = spark.read.format("csv").option("header", "true").load("/path/to/data.csv")
df_filtered = df.filter(~col("column_name").isNull())
df_stats = df_filtered.select(
    count("*").alias("total_records"),
    avg("numeric_column").alias("average_value"),
    min("numeric_column").alias("min_value"),
    max("numeric_column").alias("max_value")
)
display(df_stats)
```

### Organization and Reuse
Proper notebook structuring allows for:

- Creation of reusable function libraries
- Modularization of code for simplified maintenance
- Sharing components between projects
- Integrated and self-explanatory documentation

## Azure Databricks Workflow

### 1. Cluster Creation and Configuration

The process begins with creating clusters optimized for specific project needs:

```python
# Programmatic cluster configuration via API
from databricks.sdk import WorkspaceClient
from databricks.sdk.service import compute

w = WorkspaceClient()

cluster_config = compute.ClusterSpec(
    cluster_name="MyDatabricksCluster",
    spark_version="11.3.x-scala2.12",
    node_type_id="Standard_DS3_v2",
    autoscale=compute.AutoScale(min_workers=1, max_workers=5),
    spark_conf={
        "spark.databricks.delta.preview.enabled": "true",
        "spark.databricks.io.cache.enabled": "true"
    },
    autotermination_minutes=60
)

cluster = w.clusters.create(cluster_config)
print(f"Cluster created with ID: {cluster.cluster_id}")
```

### 2. File Import and Organization

Files can be imported via the interface or programmatically:

```python
# Importing files to DBFS
dbutils.fs.cp("file:/local/path/file.csv", "dbfs:/FileStore/datasets/file.csv")

# Listing imported files
files = dbutils.fs.ls("dbfs:/FileStore/datasets/")
display(files)
```

### 3. Versioning with Git and Azure DevOps

Integration with Git repositories for version control:

```bash
# Commands executed in the Databricks terminal
git init
git remote add origin https://dev.azure.com/my-org/my-project/_git/databricks-notebooks
git add .
git commit -m "Initial notebook version"
git push -u origin master
```

In the Databricks notebook:
```python
# Checking repository status
%sh
git status
git log --oneline -5
```

### 4. CI/CD Automation with Azure DevOps

Example YAML pipeline for automation:

```yaml
# azure-pipelines.yml
trigger:
- main

pool:
  vmImage: "ubuntu-latest"

steps:
- task: UsePythonVersion@0
  inputs:
    versionSpec: "3.8"
    addToPath: true

- script: |
    pip install databricks-cli pytest
    databricks configure --token
    echo $(DATABRICKS_TOKEN) | databricks configure --token
  displayName: "Configure Databricks CLI"

- script: |
    databricks workspace import_dir ./notebooks /Shared/Project
  displayName: "Deploy notebooks"

- script: |
    pytest ./tests
  displayName: "Run tests"
```

### 5. Executing Notebooks with Parameters

Parameterized execution for flexibility:

```python
# Defining widgets for parameters
dbutils.widgets.text("start_date", "2023-01-01", "Start Date")
dbutils.widgets.text("end_date", "2023-12-31", "End Date")

# Retrieving parameters
start_date = dbutils.widgets.get("start_date")
end_date = dbutils.widgets.get("end_date")

# Using parameters in the query
df = spark.sql(f"""
SELECT *
FROM data_table
WHERE date BETWEEN \'{start_date}\' AND \'{end_date}\'
""")

display(df)
```

### 6. Interactive Visualizations and Analysis

Creating advanced visualizations:

```python
# Importing visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Data preparation
results = spark.sql("""
SELECT category, SUM(value) as total
FROM sales
GROUP BY category
ORDER BY total DESC
LIMIT 10
""").toPandas()

# Creating visualization
plt.figure(figsize=(10, 6))
sns.barplot(x=\'category\', y=\'total\', data=results)
plt.title(\'Total Sales by Category\')
plt.xticks(rotation=45)
plt.tight_layout()
display()
```

## Learning Resources

Microsoft Learn offers valuable resources to deepen knowledge in Azure Databricks:

- [Introduction to Azure Databricks](https://learn.microsoft.com/en-us/training/modules/intro-to-azure-databricks/)
- [Data Engineering with Azure Databricks](https://learn.microsoft.com/en-us/training/paths/data-engineer-azure-databricks/)
- [Data Science with Azure Databricks](https://learn.microsoft.com/en-us/training/paths/perform-data-science-azure-databricks/)

## Conclusion

Azure Databricks, when combined with appropriate versioning and code organization practices, provides a robust and collaborative environment for data projects. Integration with Azure DevOps and the use of AI for development assistance enhance productivity and notebook quality, allowing teams to work more efficiently on complex data analysis, data engineering, and machine learning projects.


## üìã Description

Describe the content of this section here.


## üì¶ Installation

Describe the content of this section here.


## üíª Usage

Describe the content of this section here.


## üìÑ License

Describe the content of this section here.

